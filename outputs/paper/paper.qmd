---
title: "TBD"
title-block-banner: true
abstract: "TBD"
thanks: "Code and data supporting this analysis is available at: https://github.com/christina-wei/TBD.git"
author: Christina Wei
date: today
date-format: long
format: pdf
editor: visual
toc: true
number-sections: true
bibliography: ref.bib
---

```{r}
#| message: false
#| echo: false
#| warning: false

#### Workspace set-up and read in data ####

# Load packages 
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(viridis)

source(here("scripts/02-model.R"))
source(here("scripts/03-helper_functions.R"))

# read in cleaned data

cleaned_time_sheet = read_csv(
  file = here("outputs/data/cleaned_time_sheet.csv"),
  show_col_types = FALSE
)

# set factor
cleaned_time_sheet$type =
  factor(cleaned_time_sheet$type,
         levels = c(
           "Research",
           "Course Work",
           "TA",
           "Admin",
           "Learning",
           "Other Activities"
         )
  )
```

```{r}
#| message: false
#| echo: false
#| warning: false

## Variations of timesheet data

# Calculate daily data
daily_data <- 
  cleaned_time_sheet |>
  mutate(wkday = wday(cleaned_time_sheet$start_date, label = TRUE)) |>
  group_by(start_date, wkday, type) |>
  summarize(effort = sum(effort_hours)) |>
  ungroup()

# Calculate data for hours of each day
hourly_calc <- 
  cleaned_time_sheet |>
  slice(rep(1:n(), each = 24)) |>
  mutate(hour_value = rep(0:23, times = nrow(cleaned_time_sheet))) |>
  mutate(
    worked = (start_date == end_date & 
                hour(start_time) <= hour_value & 
                convert_hms_to_hours(end_time) > hour_value
              ) |
              (start_date < end_date & 
                 (convert_hms_to_hours(end_time) > hour_value |
                    hour(start_time) <= hour_value)
               )
    ) |>
  filter(worked) |>
  select(-worked)
```

# Introduction

TBD

# Data

## Source & Data Cleaning

The timesheet data used for this analysis is based on task tracking from August 15, 2022 to March 13, 2023. I used the an mobile application Toggl Track[^1] on my phone to keep track of my academic activities every day. There are in total 581 time entries tracked for the given time period.

[^1]: https://toggl.com/track/

```{r}
#| message: false
#| echo: false
#| warning: false
#| include: false

min(cleaned_time_sheet$start_date)
max(cleaned_time_sheet$end_date)
nrow(cleaned_time_sheet)
```

The following information is tracked for each time entry:

-   `Description` of the activity that is being tracked

-   `Project` where the activity belongs to, such as class code (e.g. INF3104) or research project (e.g. Lit Review). Sometimes this value is not populated in the timesheet

-   `Start date`, `start time`, `end date`, and `end time` of the tracked activity

The raw timesheet data is exported directly from the Toggl Track application. I applied data cleaning routine to include additional information on the type of work, as well as the effort duration in hours for each entry. The `type` of work is defined as:

-   Course Work - time spent on course related activities, like attending classes, working on assignments

-   TA - time spent on teaching assistant obligations, like marking assignments and

-   Research - activities related to research, including reading papers, thematic analysis, and writing

-   Admin - administrative tasks like applying for grants, filling in timesheets

-   Learning - learning activities outside of coursework, like workshops, writing help, orientations

-   Other Activities - catches miscellaneous activities like lab meetings, updating personal websites

Data was cleaned and analyzed using the open source statistically programming language R [@r], using functionalities from `tidyverse` [@rTidyverse], `ggplot2` [@rGgplot2], `dplyr` [@rDplyr], `readr` [@rReadr], `tibble` [@rTibble], `here` [@rHere], `kableExtra` [@rKableExtra] and `knitr` [@rKnitr].

Please see @tbl-sample for a sample of the timesheet data.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-sample
#| tbl-cap: Sample of the timesheet data
 
cleaned_time_sheet |>
  tail() |>
  kable(
    format = "latex",
    booktabs = TRUE,
    digits = 1
  ) |>
  kable_styling(latex_options = "scale_down")

```

## Data Limitations

As the timesheet data depends on my action to manually tracked my time accurately, there is definitely the potential for *human errors* in the data. Specifically, there may be missing data that I forgot to enter into the application. Also, some data are entered retrospectively after the actual activity happened, which could impact the accuracy of time tracked. Also, there are potential issues of tracking time to the wrong project or description that's not reflective of the actual activity.

The selected *granularity* of data used in timesheet tracking is also a potential limitation. There are aspects that are not currently captured which may be useful for analysis. For example, data can be enriched by tracking productivity, location, and more details on the activities for each session. Some of these attributes are not tracked because they do not have a field on the application to be entered. But even if the additional fields are available on the app, there is also the consideration to balance the ease of performing data tracking vs. the amount of data to be tracked. If the act of tracking data becomes cumbersome to do, I may stop tracking time all together to avoid the hassle, which results in more loss in data than not having more data attributes.

There is also an interesting limitation on the *consistency* of project categorization over time as more knowledge is gained on how to efficiently organize projects. For example, my one-on-one meetings with my supervisor were not categorized into any projects initially because I wasn't sure where to put them. After a few months, I created the project "Research-General" to capture general activities related to research, and have tagged future one-on-one meetings into this category. For this particular situation, the data cleaning routine moved the untagged 1:1 meetings into the "Research-General" project. However, there may be other activities with different projects over time that were not adjusted.

\<Data not complete - before and after\>

-   Sleep data - automatically tracked through Fitbit

-   Deliverable data - created at the beginning of each term to track major deliverables

# Results

## Type of Work

From August 15, 2022 to March 13, 2023, I have tracked 1034 hours over 581 sessions. Looking at @fig-time-spent, most of my time are spent on Research (398 hours, 38%) and Course Work (343 hours, 33%) type of work. Performing teaching assistant duties takes about 12% of my time with 124 hours. The remaining 17% of my time are pretty equally split across Learning, Admin, and Other Activities.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-spent
#| fig-cap: Amount of time spent on each type of activity

# Piecharts: https://r-charts.com/part-whole/pie-chart-ggplot2/
cleaned_time_sheet |>
  group_by(type) |>
  summarize(effort = sum(effort_hours)) |>
  mutate(effort_pct = effort / sum(effort)) |>
  ggplot(aes(x = "", y = effort_pct, fill = fct_rev(type))) +
  geom_col(color = "black") +
  geom_label(aes(label = round(effort, 0)),
             position = position_stack(vjust = 0.5),
             size = 3,
             show.legend = FALSE) +
  coord_polar(theta = "y") +
  labs(
    y = "Effort (Hours)",
    fill = "Type"
  ) +
  scale_fill_viridis(discrete = TRUE) #colorblind-friendly palette
```

To understand better the time spent on Research and Course Work, let's drill down to the project level details. Looking at Research time specifically, @fig-time-spent-bytype-1 shows that a significant amount of time is spent on working on the literature review paper (Lit Review) at 253 hours, which is the first research paper that I worked on as the lead author. The marginalization in financial industry (Finance CA Marginalization) is a workshop paper I authored, where I spent 21 hours working on the manuscript. The projects on older adults' perception on conversational agents (OAPerception), industry digital design marginalization (Industry DDN) and perceptions of conversational agents with disfluency (Disfluency perception) do not have complete data, as work done before August 15, 2022 was not tracked. The "Other" category shows that I spent 21 hours in the tracked period on general research activities, such as reading literature to keep up to date with research.

Looking at time spent on Course Work in @fig-time-spent-bytype-2 , there are two courses that are taking significant amount of time: INF2241 Critical Making at 142 hours and INF3104 Data Science Foundations at 103 hours. Both of these courses are technical in nature, as the course work requires both programming skills as well as written communication skills to summarize findings. It is worth noting that INF3104 and INF2169 are courses that I am currently taking, and there will be more time spent on them that will not reflected in this analysis.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-spent-bytype
#| fig-cap: Break down time spent for Research and Course Work
#| fig-subcap: ["Research","Course Work"]
#| layout-ncol: 2
#| layout-nrow: 1

generate_compare_bar_graph(
  ggplot_data = 
    cleaned_time_sheet |>
    filter(type == "Research") |>
    mutate(project = ifelse(project %in% c(NA, "Reading", "Research-General"),
                            "Other",
                            project)
           ) |>
    group_by(project) |>
    summarize(effort = sum(effort_hours)) |>
    ggplot(aes(x = factor(project, 
                          level = c("Lit Review", 
                                    "OAPerception", 
                                    "Finance CA Marginalization", 
                                    "Disfluency perception", 
                                    "Industry DDN",
                                    "Other")),
               y = effort)),
  position = "identity",
  angle = 45,
  vjust = 1,
  hjust = 1,
  xlabel = "Project",
  ylabel = "Effort (hours)"
) +
  geom_text(aes(label = round(effort, 0)), vjust = -0.2) +
  ylim(0, 260)

generate_compare_bar_graph(
  ggplot_data = 
    cleaned_time_sheet |>
    filter(type == "Course Work") |>
    group_by(project) |>
    summarize(effort = sum(effort_hours)) |>
    ggplot(aes(x = factor(project, 
                          level = c("INF2241", "INF3104", "INF3001", "INF2169")), 
               y = effort)),
  position = "identity",
  xlabel = "Course",
  ylabel = "Effort (hours)"
) +
  geom_text(aes(label = round(effort, 0)), vjust = -0.2) +
  ylim(0, 145)
```

## Session Patterns

The average time spent on each session is 1.8 hours, with a standard deviation of 1.1 hours. The minimum amount of time spent on a session is \~10 minutes on TA marking activities, while the maximum amount of time spent in one session is 10.7 hours working on paper 1 for INF3104. (see @tbl-session-summary for summary statistics)

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-session-summary
#| tbl-cap: Summary statistics for each session

cleaned_time_sheet |>
  summarize(
    "Avg Session Time" = mean(effort_hours),
    "SD Session Time" = sd(effort_hours),
    "Min Session Time" = min(effort_hours),
    "Max Session Time" = max(effort_hours)
    ) |>
  kable(
    booktabs = TRUE,
    align = "c",
    digits = 1
  )
```

Looking at the **length of time** spent per session, @fig-session-bylength-1 shows that the typical length of sessions are less than 3 hours (87%), with most of them within the 1 to 2 hours range (41%). It is rare to have sessions over 4 hours long (\<5%). Breaking down the sessions by type of work, we see in @fig-session-bylength-2 that the medians of the session for each type are not significantly different, and are within the range of 1-2 hours per session. Course work and other activities have some large outliers in data that are over 6 hours long.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-session-bylength
#| fig-cap: Visualize sessions by length of time
#| fig-subcap: ["Percentage sessions for different lengths of time","Distribution of session length by type"]
#| layout-ncol: 2
#| layout-nrow: 1

generate_compare_bar_graph(
  ggplot_data = 
    cleaned_time_sheet |>
    filter(type %in% c("TA", "Research", "Course Work", "Learning")) |>
    mutate(effort_bin = case_when(
      effort_hours < 1 ~ "< 1",
      effort_hours >= 1 & effort_hours < 2 ~ "1 - 2",
      effort_hours >= 2 & effort_hours < 3 ~ "2 - 3",
      effort_hours >= 3 & effort_hours < 4 ~ "3 - 4",
      effort_hours >= 4 & effort_hours < 5 ~ "4 - 5",
      effort_hours >= 5 ~ "> 5"
    )) |>
    group_by(effort_bin, type) |>
    count() |>
    ungroup() |>
    mutate(pct = n / sum(n)) |>
    ggplot(aes(x = factor(effort_bin, 
                          level = c("< 1", "1 - 2", "2 - 3", "3 - 4", "4 - 5", "> 5")), 
               y = pct, 
               fill = type)),
  position = "stack",
  yscale = scales::percent,
  xlabel = "Length of session (in hours)",
  ylabel = "Percentage of occurrence"
)

cleaned_time_sheet |>
  ggplot(aes(x = type, y = effort_hours)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.3, alpha = 0.4) +
  labs(
    x = "",
    y = "Effort per session (hours)"
  ) +
  scale_y_continuous(breaks = seq(0, 11, 1)) +
  theme_minimal()
```

Let's take a look at the outlier sessions (\>4 hours) for each type of work (@tbl-over-4hrs):

-   *Research* - there are 11 outlier sessions, clustered mostly between 4-5 hours, with the maximum length at 5.7 hours

-   *Course Work* - there are 9 outlier sessions, with a couple of large outliers at 10.7 hours and 6.5 hours. All the outlier sessions are related to courses INF2241 and INF3104

-   *TA* - there are 2 outlier sessions, both are related to assignment mark with length around 4 hours

-   *Admin* - there are 3 outlier sessions, all of them related to scholarship applications

-   *Learning* - the 1 outlier is the Critical Computing workshop that lasted 5.5 hours

-   *Other Activities* - there are 3 outlier sessions, two of them (6 and 7.5 hours) are related to the Data Science Hackathon that I participated as a mentor for full days. The other outlier is spending 4 hours updating my resume

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-over-4hrs
#| tbl-cap: Summary statistics for sessions that are over 4 hours long

cleaned_time_sheet |>
  filter(effort_hours > 4) |>
  group_by(type) |>
  summarize(
    count = n(),
    avg_effort = mean(effort_hours),
    max_effort = max(effort_hours)
  ) |>
  mutate(work_performed = c(
    "data analysis, paper writing, paper review",
    "assignments from INF2241, INF3104",
    "CCT419 (UX and board games) marking",
    "NSERC application, SRI scholarship",
    "Critical Computing - qualitative analysis workshop",
    "Data Science Hackathon, update resume"
  )) |>
  arrange(type, desc(count)) |>
  kable(
    booktabs = TRUE,
    digits = 1,
    linesep = "",
    align = 'lcccl',
    col.names = c("Type", "Count", "Avg Effort", "Max Effort" ,"Tasks")
  )
```

To understand the patterns for **hours of the day** that I am working, @fig-session-byhour visualizes the percentage of sessions that are occurring during each hour of the day. Overall, it looks most of the working time are between 8am and midnight, with two periods of time that have the most frequent number of work sessions: between 11am to 4pm, and between 9pm and midnight. The earliest I started working is around 8am, and the latest that I finished work is around 4am. As for hours of the day by different types of work, it looks like Course Work takes a larger proportion of sessions in the early afternoons (12pm - 3pm), and Research takes a larger proportion in late evenings (9pm to midnight).

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-session-byhour
#| fig-cap: Percentage of sessions occurring during each hour of the day

generate_compare_bar_graph(
  ggplot_data =
    hourly_calc |>
    #filter(type %in% c("Course Work", "Learning", "Research", "TA")) |>
    group_by(hour_value, type) |>
    count() |>
    ungroup() |>
    mutate(pct = n / sum(n)) |>
    ggplot(aes(x = hour_value, y = pct, fill = type)),
  xlabel = "Hour of the Day",
  ylabel = "Percentage of Occurrence",
  position = "stack",
  yscale = scales::percent
)
```

## Daily Patterns

The average time spent on work each day is 5.7 hours, with a standard deviation of 2.5 hours. The minimum amount of time that I worked in a day was 0.5 hours, while the maximum amount of time working in one day is 12.5 hours. On average I worked on 2 different projects each day. The minimum number of projects I worked on in a day is 1, while the maximum number of projects I worked in a day is 6 (see @tbl-day-summary for summary statistics).

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-day-summary
#| tbl-cap: Summary statistics for each day on number of sessions and effort

tmp_daily_effort <-
  daily_data |>
  group_by(start_date) |>
  summarize(effort = sum(effort))

tmp_daily_projects <-
  cleaned_time_sheet |>
  group_by(start_date, project) |>
  summarize(projects = n()) |>
  group_by(start_date) |>
  summarize(projects = n()) 

tmp_daily <- merge(tmp_daily_effort, tmp_daily_projects, by="start_date")
rm(tmp_daily_effort)
rm(tmp_daily_projects)

tibble(
  Statistics = c("Mean", "SD", "Min", "Max"),
  "Hours Worked" = c(mean(tmp_daily$effort),
                     sd(tmp_daily$effort),
                     min(tmp_daily$effort),
                     max(tmp_daily$effort)),
  "Number of Projects" = c(as.integer(mean(tmp_daily$projects)),
                           as.integer(sd(tmp_daily$projects)),
                           as.integer(min(tmp_daily$projects)),
                           as.integer(max(tmp_daily$projects))),
) |>
  kable(
    booktabs = TRUE,
    digits = 1,
    align = 'c'
  )
```

### Hours Worked Per Day

Looking at the distribution of the hours worked per day @fig-day-effort-1, most of the values are centered around the mean of 5.7 hours between 4 - 8 hours per day. There are some extra long days in the dataset, with close to 10 days that I have worked for over 10 hours per day. There are usually right before a specific deadline for submitting a grant application or a large assignment. Assessing the trend for hours worked on each day of the week in @fig-day-effort-2, it looks like I spend slightly more time working from Monday to Thursday with around 6 - 7 hours a day, a slightly less time working on weekends from Friday to Sunday with around 4 - 5 hours a day.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-day-effort
#| fig-cap: Hours worked each day
#| fig-subcap: ["Distribution of hours worked per day","Hours worked per day for each day of the week"]
#| layout-ncol: 2
#| layout-nrow: 1

tmp_daily |>
  ggplot(aes(x = effort)) +
  geom_histogram(binwidth = 1, boundary = 0, color = "white") +
  theme_minimal() +
  labs(
    x = "Number of Hours Per Day",
    y = "Number of occurrences"
  ) +
  scale_x_continuous(breaks = seq(0, 13, by = 1))

tmp_daily |>
  mutate(wkday = wday(start_date, label = TRUE)) |>
  ggplot(aes(x = wkday, y = effort)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.3, alpha = 0.4) +
  labs(
    x = "Day of the Week",
    y = "Number of Hours Per Day"
  ) +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 13, by = 1))
```

### Number of Projects Per Day

It will also be interesting to understand how many **different projects** I am working on each day. Analyzing my timesheet overall (@fig-day-numprojects-1), I am usually working on between 1 to 3 projects per day, with the maximum of 6 projects per day. Drilling down into the data, there are some differences in the number of projects based on the day of the week (@fig-day-numprojects-2). It looks like Saturdays and Sundays are mostly focused on 1 or 2 projects, while on weekdays I work on a higher number of projects per day, mostly between 2 to 3 projects.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-day-numprojects
#| fig-cap: Numbers of projects worked on per day
#| fig-subcap: ["Distribution of number of projects per day","Number of projects per day for each day of the week"]
#| layout-ncol: 2
#| layout-nrow: 1

cleaned_time_sheet |>
  group_by(start_date, project) |>
  summarize(projects = n()) |>
  group_by(start_date) |>
  summarize(projects = n()) |> 
  group_by(projects) |>
  count() |>
  ungroup() |>
  mutate(pct = n / sum(n)) |>
  ggplot(aes(x = projects, y = pct)) +
  geom_bar(stat = "identity") +
  labs(
    x = "Number of Projects Worked on Per Day",
    y = "Percentage of occurrences"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)


cleaned_time_sheet |>
  group_by(start_date, project) |>
  summarize(projects = n()) |>
  group_by(start_date) |>
  summarize(projects = n()) |>
  ungroup() |>
  mutate(wkday = wday(start_date, label = TRUE)) |>
  ggplot(aes(x = wkday, y = projects)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.3, alpha = 0.4) +
  labs(
    x = "Day of the week",
    y = "Number of projects per day"
  ) +
  theme_minimal()
```

As a student who is currently in the course work portion of my PhD, my schedule is driven by the courses I am taking per semester. Therefore, I analyzed the data based for each semester to see if there are differences in the number of projects per day. Comparing the graphs between Fall 2022 (@fig-day-numprojects-semester-1) and Winter 2023 (@fig-day-numprojects-semester-2) semesters, there are noticeable differences during the weekdays between the two. In Fall 2022, Thursdays stand out as an outlier with a higher number of projects per day, with many of them working on 6 projects. In Winter 2023, Mondays are pretty consistent at 3 projects a day, while Fridays have a wider distribution of values. Weekends do not seem different between the two semesters, working on 1-2 projects per day.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-day-numprojects-semester
#| fig-cap: Numbers of projects worked on per day for each semester
#| fig-subcap: ["Fall 2022 (Sep - Dec)","Winter 2023 (Jan - Mar)"]
#| layout-ncol: 2
#| layout-nrow: 1

cleaned_time_sheet |>
  filter(start_date > as.Date("2022-09-01") & start_date <= as.Date("2022-12-15")) |>
  group_by(start_date, project) |>
  summarize(projects = n()) |>
  group_by(start_date) |>
  summarize(projects = n()) |>
  ungroup() |>
  mutate(wkday = wday(start_date, label = TRUE)) |>
  ggplot(aes(x = wkday, y = projects)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.3, alpha = 0.4) +
  labs(
    x = "Day of the week",
    y = "Number of projects per day"
  ) +
  theme_minimal()

cleaned_time_sheet |>
  filter(start_date > as.Date("2023-01-09")) |>
  group_by(start_date, project) |>
  summarize(projects = n()) |>
  group_by(start_date) |>
  summarize(projects = n()) |>
  ungroup() |>
  mutate(wkday = wday(start_date, label = TRUE)) |>
  ggplot(aes(x = wkday, y = projects)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.3, alpha = 0.4) +
  labs(
    x = "Day of the week",
    y = "Number of projects per day"
  ) +
  theme_minimal()
```

## Trend Over Time

Analyzing my weekly effort over time (@fig-overtime), I can see a slight upward trend over time as I am spending more time overall on academic matters. Most of the increase comes from efforts spent on research related work in 2023. Also, there are three different semesters that are captured here: Summer 2022, Fall 2022, and Winter 2023. Each semester seems to have a different pattern. In Summer 2022, I was spending about 20 hours a week mostly focused on research projects. In Fall 2023, the majority of my time are spent on course work and TA related activities. In this semester, Winter 2023, most of my time are spent on research and course work activities. There is also a noticeable decrease in time in December, where I took a break over the holidays.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-overtime
#| fig-cap: Weekly effort over time

# Weekly trend
# https://stackoverflow.com/questions/67847174/convert-daily-data-into-weekly-data-and-summarize-multiple-columns-in-r

# bar graph comparison over time
generate_compare_bar_graph(
  ggplot_data =
    cleaned_time_sheet |>
    mutate(type = ifelse(type %in% c("Research", "Course Work", "TA"), 
                         as.character(type), 
                         "Other")) |>
    group_by(wk_index = floor_date(start_date, "week"), type) |>
    summarize(effort = sum(effort_hours)) |>
    ggplot(aes(x = wk_index, 
               y = effort, 
               fill = factor(type, level = c("Research", "Course Work", "TA", "Other")))),
  position = "stack", 
  xlabel = "Week",
  ylabel = "Weekly Effort (hours)",
  fill_label = "Project",
) +
  geom_vline(xintercept = as.Date("2022-09-08"), linetype = "dotted") +
  geom_vline(xintercept = as.Date("2022-12-14"), linetype = "dotted") +
  geom_vline(xintercept = as.Date("2023-01-05"), linetype = "dotted") +
  scale_x_date(date_breaks = "1 months", date_labels = "%b-%y")
```

### Course Work & TA

As both course work and TA are structured based on each semester, their trends are analyzed together in this section. Looking at the trend over time for course work for both Fall 2022 and Winter 2023 semester in @fig-overtime-course-ta-1, it looks like there is a pattern where there are more work at the beginning and the end of the semester, with some down time in the middle. As for TA work ( @fig-overtime-course-ta-2), while the workload is mostly steady at a few hours a week, there are noticeable peaks when a large amount of time is needed for fast turnaround on deliverables.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-overtime-course-ta
#| fig-cap: Weekly effort for differe type of work
#| fig-subcap: ["Course Work","Teaching Assistant"]
#| layout-ncol: 2
#| layout-nrow: 1

generate_compare_bar_graph(
    ggplot_data = 
      cleaned_time_sheet |>
      filter(type == "Course Work") |>
      group_by(wk_index = floor_date(start_date, "week")) |>
      summarize(effort = sum(effort_hours)) |>
      ggplot(aes(x = wk_index, y = effort)),
    xlabel = "Week",
    ylabel = "Effort (in hours)",
    position = "stack", 
  ) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b-%y")

generate_compare_bar_graph(
    ggplot_data = 
      cleaned_time_sheet |>
      filter(type == "TA") |>
      group_by(wk_index = floor_date(start_date, "week")) |>
      summarize(effort = sum(effort_hours)) |>
      ggplot(aes(x = wk_index, y = effort)),
    xlabel = "Week",
    ylabel = "Effort (in hours)",
    position = "stack", 
  ) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b-%y")
```

### Research

Since August 2022, I have been spending on average of 14.2 hours per week on research related activities. There is high variation (standard deviation = 11.3 hours) in the weekly effort on research, with minimum of 1.1 hours and maximum of 51 hours (see @tbl-research). The variation is correlated with the submissions deadlines for conferences and journals, as the number of hours spent on research increases dramatically in the weeks leading up to the deadline (@fig-research). One good example is the time spent on the systematic literature review paper (Lit Review), where there are a couple of weeks with over 40 hours before the deadline.

Based on the graph in @fig-research, there are usually only 1 or 2 research projects that I'm working on at a given time. As a research project finishes, then I can take on new projects for research. This rolling schedule can be seen with the OA Perception project as the main focus in August and September. After this project is finished, the next major project, Lit Review is started in October and continue to be the main focus until end of February. It is also interesting to observe the differences between being the lead author on Lit Review where there are high peaks of writing time before submission versus a contributing researcher on OA Perception where it is a steady amount of work before the deadline.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-research
#| tbl-cap: Summary statistics for weekly effort on research related activities

cleaned_time_sheet |>
  filter(type == "Research") |>
  group_by(wk_index = floor_date(start_date, "week")) |>
  summarize(effort = sum(effort_hours)) |>
  ungroup() |>
  summarize(
    mean(effort), sd(effort), min(effort), max(effort),
  ) |>
  kable(
    booktabs = TRUE,
    digits = 1,
    align = "c",
    col.names = c("Mean", "SD", "Min", "Max")
  )
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-research
#| fig-cap: Weekly effort for differe type of work

generate_compare_bar_graph(
    ggplot_data = 
      cleaned_time_sheet |>
      filter(type == "Research") |>
      mutate(project = ifelse(project %in% c("Disfluency perception",
                                             "Finance CA Marginalization", 
                                             "Lit Review", 
                                             "OAPerception"), 
                              project, 
                              "Other")) |>
      group_by(wk_index = floor_date(start_date, "week"), project) |>
      summarize(effort = sum(effort_hours)) |>
      ggplot(aes(x = wk_index, y = effort, fill = project)),
    xlabel = "Week",
    ylabel = "Effort (in hours)",
    fill_label = "Project",
    position = "stack",
    nrow = 2
  ) +
  scale_x_date(date_breaks = "1 months", date_labels = "%b-%y")
```

# Model

There are two prediction models created to help me with time management: one is to predict the number of hours available working on a deliverable based on a given targeted delivery date; the other is based on the estimated effort in hours to project the estimated completion date. Also, a calibration routine is used to control the model parameters used in calibration. These models assume that weekly patterns of hours spent working per day in the past is representative of future behavioural working patterns.

## Calibrate Model Parameters

The model parameter used in the prediction models are based on the average hours spent working for each day of the week. There are some optional inputs that users can use to filter timesheet data: using start date and end date to select a specific period, or to specific the type of work (e.g. Research) to be used in calibration. The timesheet data is then used to compute the average number of hours per day, which is then summarized by calculating the average hours for each day of the week.

Here is an example of invoking the calibration routine based on all available timesheet data with the output model parameters in @tbl-calibration-all.

``` r
calibrate_model()
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-calibration-all
#| tbl-cap: Sample of model parameter output with no filtering criteria

calibrate_model() |>
  kable(
    booktabs = TRUE,
    linesep = ""
  )
```

Here is another example of invoking the calibration routine based on timesheet data from January 9 to April 1, 2023, using only entries related to Research type of work. The corresponding model parameters are shown in @tbl-calibration-research.

``` r
calibrate_model(as.Date("2023-01-09"), as.Date("2023-04-01"), "Research")
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-calibration-research
#| tbl-cap: Sample of model parameter output filtered from Jan 9 to Apr 1, 2023 and Research

calibrate_model(as.Date("2023-01-09"), as.Date("2023-04-01"), "Research") |>
  kable(
    booktabs = TRUE,
    linesep = ""
  )
```

## Predict Hours Available Between Dates

This function is used to estimate the number of hours I have available to work on a project if I have a target completion date in mind. As such, the input parameters for this function are defined as the following:

-   `model_param` specifies the calibrated number of hours worked for each day of the week (e.g. @tbl-calibration-all)

-   `start_date(optional)` specifies the start date of the project. If not specified, it will default to today's date

-   `end_date` specifies the target completion date of the project

-   `pct_effort(optional)` specifies the multiplier to be applied to the calibrate hours in model parameters. For example, specifying 1.2 here will scale up the calibrated hours by 120%.

-   `include_weekends(optional)` specifies whether the estimated hours should include Saturdays and Sundays. If not specified this value is default to be TRUE to include weekends in prediction.

To estimate the number of hours, the algorithm is projecting the list of dates that are available between the start date and the end date, calculating the number of days available for each day of the week (e.g. 3 Mondays, 2 Tuesdays ..), multiplying them by the average number of hours for each day of the week, then scaling them up by the `pct_effort` parameter to generate the final estimate.

$$
total\_hours = \sum_{i=1}^{N} \beta_{i} * n_{i} * \alpha
$$ {#eq-predict-hours}

The parameters in @eq-predict-hours are:

-   i = day of the week (e.g. 1 = Mon, 2 = Tues ..)

-   N = 1 to 7 if include_weekends = TRUE; 1 to 5 if include_weekends = FALSE

-   $\beta_{i}$ = average hours for the day of the week

-   $n_{i}$ = number of the day of the week (e.g. 3 Mondays)

-   $\alpha$ = pct_effort multiplier value for effort scaling

For example, the following code snippet estimates the number of hours I have available between now and April 17 based all the timesheet data available. Based on the prediction model, I have 106 hours available for work, and it also generates a detailed daily schedule for my review (sample schedule in @tbl-predict-hours).

``` r
model_param <- calibrate_model()
predict_hours_available(model_param, end_date = as.Date("2023-04-17"))
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-predict-hours
#| tbl-cap: Sample output of detailed daily schedule from predict hours available algorithm

predict_hours_available(calibrate_model(), end_date = as.Date("2023-04-17"))$schedule |>
  head() |>
  kable(
    booktabs = TRUE,
    linesep = ""
  )
```

## Predict Completion Date

On the other hand, I can also predict the estimated completion date if I have an estimated number of effort for a project. The algorithm of this prediction is similar to predicting the hours available between two date by using the average number of hours worked for each day of the week as the baseline. Specifically, the input parameters for this function are defined as the following:

-   `model_param` specifies the calibrated number of hours worked for each day of the week (e.g. @tbl-calibration-all)

-   `start_date(optional)` specifies the start date of the project. If not specified, it will default to today's date

-   `effort_hours` specifies the estimated number of hours for the project

-   `pct_effort(optional)` specifies the multiplier to be applied to the calibrate hours in model parameters. For example, specifying 1.2 here will scale up the calibrated hours by 120%.

-   `include_weekends(optional)` specifies whether the estimated hours should include Saturdays and Sundays. If not specified this value is default to be TRUE to include weekends in prediction.

To estimate the completion date, the algorithm generates a sequence of daily dates from start date based on criteria of whether to include weekend dates, then populating each date by the estimated hours for each day of the week multiplied by the `pct_effort` parameter. When the cumulative sum of the daily efforts exceeds the estimate number of hours for the project, then the routine is stopped with the final date in the sequence returned as the estimated date of completion.

As an example, I would like to use this prediction model to help me estimate the date that I will be completing the Final Paper for the Data Science Foundations course. I would like to use this semester's timesheet related to course work to estimate my behavioural patterns. I am estimating the remaining effort on the paper is 10 hours, and I can spend about 50% of my total efforts working on this. Lastly, I do not want to work on weekends. The code snippet below demonstrates the translation of my requirements into parameters for calibration and prediction models. The model predicts that I will be able to complete the assignment on April 7, 2023 with the detailed schedule generated in @tbl-predict-date.

``` r
model_param <- calibrate_model(
  from_date = as.Date("2023-01-09"),
  to_date = as.Date("2023-03-30"),
  work_type = "Course Work")

predict_completion_date(
  model_param, 
  effort_hours = 10,
  pct_effort = 0.5,
  include_weekends = FALSE)
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-predict-date
#| tbl-cap: Sample output of detailed daily schedule from predict completion date algorithm

predict_completion_date(
  calibrate_model(as.Date("2023-01-09"), as.Date("2023-03-30"), "Course Work"),
  start_date = as.Date("2023-03-31"),
  effort_hours = 10,
  pct_effort = 0.5, 
  include_weekends = FALSE)$schedule |>
  kable(
    booktabs = TRUE,
    linesep = ""
  )
```

# Discussion

-   Missing - social aspects, quality of work

-   Weekends / timeoff / work life balance

-   Different types of flow - technical vs. research

-   Find the focus each day - try to stay with 1 or 2 topics only

-   Prefer to work on morning / early afternoon, and after dinner

-   Time fixed by events - like going to school, meetings that I do not control

## Future Improvements

The prediction models are built on the assumptions that past behavioural patterns can be used to predict future patterns. However, this may not always hold true. For example, I am currently in the course work period of my PhD studies, where a large amount of my time is driven by

# Conclusion

# Appendix {.unnumbered}

Shiny App

![](shiny_app_screenshot.png)

# Reference
